{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Week 01. Text Data Essentials<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-01.-Introduction-to-Text-Data\" data-toc-modified-id=\"Week-01.-Introduction-to-Text-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 01. Introduction to Text Data</a></span></li><li><span><a href=\"#Loading-and-Inspecting-Data-with-Pandas\" data-toc-modified-id=\"Loading-and-Inspecting-Data-with-Pandas-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading and Inspecting Data with Pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Iterating-over-documents-in-a-dataframe\" data-toc-modified-id=\"Iterating-over-documents-in-a-dataframe-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Iterating over documents in a dataframe</a></span></li><li><span><a href=\"#Saving-data\" data-toc-modified-id=\"Saving-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Saving data</a></span></li></ul></li><li><span><a href=\"#Web-Scraping\" data-toc-modified-id=\"Web-Scraping-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Web Scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#Downloading-URL's\" data-toc-modified-id=\"Downloading-URL's-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Downloading URL's</a></span></li><li><span><a href=\"#Parsing-HTML\" data-toc-modified-id=\"Parsing-HTML-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Parsing HTML</a></span></li><li><span><a href=\"#Removing-unicode-characters\" data-toc-modified-id=\"Removing-unicode-characters-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Removing unicode characters</a></span></li></ul></li><li><span><a href=\"#Quantity-of-Text\" data-toc-modified-id=\"Quantity-of-Text-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quantity of Text</a></span></li><li><span><a href=\"#Dictionary-/-Matching-Methods\" data-toc-modified-id=\"Dictionary-/-Matching-Methods-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dictionary / Matching Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Sentiment Analysis</a></span></li><li><span><a href=\"#Sentiment-Analysis-with-Huggingface\" data-toc-modified-id=\"Sentiment-Analysis-with-Huggingface-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Sentiment Analysis with Huggingface</a></span></li><li><span><a href=\"#StopWords\" data-toc-modified-id=\"StopWords-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>StopWords</a></span></li><li><span><a href=\"#RegEx\" data-toc-modified-id=\"RegEx-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>RegEx</a></span></li><li><span><a href=\"#WordNet\" data-toc-modified-id=\"WordNet-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>WordNet</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 01. Introduction to Text Data\n",
    "\n",
    "Natural Language Processing for Law and Social Science<br>\n",
    "Elliott Ash, ETH Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:17.481599Z",
     "start_time": "2022-02-21T10:00:17.479614Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import numpy as np\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Inspecting Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.090647Z",
     "start_time": "2022-02-21T10:00:17.755953Z"
    }
   },
   "outputs": [],
   "source": [
    "#import warnings; warnings.simplefilter('ignore')\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sc_cases.zip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.096783Z",
     "start_time": "2022-02-21T10:00:18.091674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820 entries, 0 to 819\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   case_name       786 non-null    object \n",
      " 1   opinion_type    820 non-null    object \n",
      " 2   date_standard   820 non-null    object \n",
      " 3   authorship      820 non-null    object \n",
      " 4   x_republican    803 non-null    float64\n",
      " 5   maj_judges      786 non-null    object \n",
      " 6   dissent_judges  786 non-null    object \n",
      " 7   topic_id        786 non-null    float64\n",
      " 8   cite_count      812 non-null    float64\n",
      " 9   opinion_text    820 non-null    object \n",
      "dtypes: float64(3), object(7)\n",
      "memory usage: 41.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.106182Z",
     "start_time": "2022-02-21T10:00:18.097689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>opinion_type</th>\n",
       "      <th>date_standard</th>\n",
       "      <th>authorship</th>\n",
       "      <th>x_republican</th>\n",
       "      <th>maj_judges</th>\n",
       "      <th>dissent_judges</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>cite_count</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERICK CORNELL CLAY v. UNITED STATES</td>\n",
       "      <td>majority</td>\n",
       "      <td>2003-03-04</td>\n",
       "      <td>GINSBURG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2926.0</td>\n",
       "      <td>JUSTICE GINSBURG delivered the opinion of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HILLSIDE DAIRY INC., A&amp;A DAIRY, L&amp;S DAIRY, AND...</td>\n",
       "      <td>majority</td>\n",
       "      <td>2003-06-09</td>\n",
       "      <td>STEVENS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>['THOMAS, CLARENCE']</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>Justice Stevens delivered the opinion of the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHARLES RUSSELL RHINES v. DOUGLAS WEBER, WARDEN</td>\n",
       "      <td>majority</td>\n",
       "      <td>2005-03-30</td>\n",
       "      <td>O'CONNOR</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23364.0</td>\n",
       "      <td>Justice O'Connor delivered the opinion of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>majority</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>KENNEDY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Justice Kennedy, Circuit Justice. \\n\\n This is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STATE OF ALASKA v. UNITED STATES OF AMERICA</td>\n",
       "      <td>majority</td>\n",
       "      <td>2005-06-06</td>\n",
       "      <td>KENNEDY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>['REHNQUIST, WILLIAM', 'SCALIA, ANTONIN', 'THO...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>Justice Kennedy delivered the opinion of the C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           case_name opinion_type  \\\n",
       "0                ERICK CORNELL CLAY v. UNITED STATES     majority   \n",
       "1  HILLSIDE DAIRY INC., A&A DAIRY, L&S DAIRY, AND...     majority   \n",
       "2    CHARLES RUSSELL RHINES v. DOUGLAS WEBER, WARDEN     majority   \n",
       "3                                                NaN     majority   \n",
       "4        STATE OF ALASKA v. UNITED STATES OF AMERICA     majority   \n",
       "\n",
       "  date_standard authorship  x_republican  \\\n",
       "0    2003-03-04   GINSBURG           0.0   \n",
       "1    2003-06-09    STEVENS           1.0   \n",
       "2    2005-03-30   O'CONNOR           1.0   \n",
       "3    2005-04-15    KENNEDY           1.0   \n",
       "4    2005-06-06    KENNEDY           1.0   \n",
       "\n",
       "                                          maj_judges  \\\n",
       "0  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "1  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "2  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "3                                                NaN   \n",
       "4  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "\n",
       "                                      dissent_judges  topic_id  cite_count  \\\n",
       "0                                                 []       1.0      2926.0   \n",
       "1                               ['THOMAS, CLARENCE']       8.0       117.0   \n",
       "2                                                 []       1.0     23364.0   \n",
       "3                                                NaN       NaN         6.0   \n",
       "4  ['REHNQUIST, WILLIAM', 'SCALIA, ANTONIN', 'THO...      10.0        84.0   \n",
       "\n",
       "                                        opinion_text  \n",
       "0  JUSTICE GINSBURG delivered the opinion of the ...  \n",
       "1  Justice Stevens delivered the opinion of the C...  \n",
       "2  Justice O'Connor delivered the opinion of the ...  \n",
       "3  Justice Kennedy, Circuit Justice. \\n\\n This is...  \n",
       "4  Justice Kennedy delivered the opinion of the C...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.114089Z",
     "start_time": "2022-02-21T10:00:18.107965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>opinion_type</th>\n",
       "      <th>date_standard</th>\n",
       "      <th>authorship</th>\n",
       "      <th>x_republican</th>\n",
       "      <th>maj_judges</th>\n",
       "      <th>dissent_judges</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>cite_count</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERICK CORNELL CLAY v. UNITED STATES</td>\n",
       "      <td>majority</td>\n",
       "      <td>2003-03-04</td>\n",
       "      <td>GINSBURG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2926.0</td>\n",
       "      <td>JUSTICE GINSBURG delivered the opinion of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HILLSIDE DAIRY INC., A&amp;A DAIRY, L&amp;S DAIRY, AND...</td>\n",
       "      <td>majority</td>\n",
       "      <td>2003-06-09</td>\n",
       "      <td>STEVENS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>['THOMAS, CLARENCE']</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>Justice Stevens delivered the opinion of the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHARLES RUSSELL RHINES v. DOUGLAS WEBER, WARDEN</td>\n",
       "      <td>majority</td>\n",
       "      <td>2005-03-30</td>\n",
       "      <td>O'CONNOR</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23364.0</td>\n",
       "      <td>Justice O'Connor delivered the opinion of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STATE OF ALASKA v. UNITED STATES OF AMERICA</td>\n",
       "      <td>majority</td>\n",
       "      <td>2005-06-06</td>\n",
       "      <td>KENNEDY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>['REHNQUIST, WILLIAM', 'SCALIA, ANTONIN', 'THO...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>Justice Kennedy delivered the opinion of the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>REGINALD A. WILKINSON, DIRECTOR, OHIO DEPARTME...</td>\n",
       "      <td>majority</td>\n",
       "      <td>2005-06-13</td>\n",
       "      <td>KENNEDY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4230.0</td>\n",
       "      <td>Justice Kennedy delivered the opinion of the C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           case_name opinion_type  \\\n",
       "0                ERICK CORNELL CLAY v. UNITED STATES     majority   \n",
       "1  HILLSIDE DAIRY INC., A&A DAIRY, L&S DAIRY, AND...     majority   \n",
       "2    CHARLES RUSSELL RHINES v. DOUGLAS WEBER, WARDEN     majority   \n",
       "4        STATE OF ALASKA v. UNITED STATES OF AMERICA     majority   \n",
       "5  REGINALD A. WILKINSON, DIRECTOR, OHIO DEPARTME...     majority   \n",
       "\n",
       "  date_standard authorship  x_republican  \\\n",
       "0    2003-03-04   GINSBURG           0.0   \n",
       "1    2003-06-09    STEVENS           1.0   \n",
       "2    2005-03-30   O'CONNOR           1.0   \n",
       "4    2005-06-06    KENNEDY           1.0   \n",
       "5    2005-06-13    KENNEDY           1.0   \n",
       "\n",
       "                                          maj_judges  \\\n",
       "0  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "1  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "2  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "4  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "5  ['BREYER, STEPHEN', 'GINSBURG, RUTH', 'KENNEDY...   \n",
       "\n",
       "                                      dissent_judges  topic_id  cite_count  \\\n",
       "0                                                 []       1.0      2926.0   \n",
       "1                               ['THOMAS, CLARENCE']       8.0       117.0   \n",
       "2                                                 []       1.0     23364.0   \n",
       "4  ['REHNQUIST, WILLIAM', 'SCALIA, ANTONIN', 'THO...      10.0        84.0   \n",
       "5                                                 []       4.0      4230.0   \n",
       "\n",
       "                                        opinion_text  \n",
       "0  JUSTICE GINSBURG delivered the opinion of the ...  \n",
       "1  Justice Stevens delivered the opinion of the C...  \n",
       "2  Justice O'Connor delivered the opinion of the ...  \n",
       "4  Justice Kennedy delivered the opinion of the C...  \n",
       "5  Justice Kennedy delivered the opinion of the C...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop missing\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.399603Z",
     "start_time": "2022-02-21T10:00:18.387140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        781\n",
       "unique        27\n",
       "top       SCALIA\n",
       "freq          86\n",
       "Name: authorship, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of label categories (e.g. judges)\n",
    "df['authorship'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.549716Z",
     "start_time": "2022-02-21T10:00:18.539240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SCALIA                                           86\n",
       "GINSBURG                                         81\n",
       "KENNEDY                                          79\n",
       "THOMAS                                           79\n",
       "BREYER                                           73\n",
       "SOUTER                                           72\n",
       "STEVENS                                          72\n",
       "O'CONNOR                                         52\n",
       "REHNQUIST                                        49\n",
       "ROBERTS                                          28\n",
       "ALITO                                            23\n",
       "Breyer                                           12\n",
       "Roberts                                          10\n",
       "Alito                                             9\n",
       "Ginsburg                                          8\n",
       "Scalia                                            8\n",
       "Thomas                                            8\n",
       "Stevens                                           7\n",
       "Kennedy                                           7\n",
       "SOTOMAYOR                                         6\n",
       "Souter                                            5\n",
       "STEVENS  AND  O'CONNOR ;  REHNQUIST ;  BREYER     2\n",
       "JUSTICE  ALITO                                    1\n",
       "BREYER ;                                          1\n",
       "Sotomayor                                         1\n",
       "ROBERTS ,  SCALIA ,  THOMAS ,  ALITO              1\n",
       "STEVENS  (IN PART),  BREYER  (IN PART)            1\n",
       "Name: authorship, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tabulations of label categories \n",
    "df['authorship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.668672Z",
     "start_time": "2022-02-21T10:00:18.662445Z"
    }
   },
   "outputs": [],
   "source": [
    "df['authorship'] = df['authorship'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.801274Z",
     "start_time": "2022-02-21T10:00:18.793458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SCALIA                                           94\n",
       "GINSBURG                                         89\n",
       "THOMAS                                           87\n",
       "KENNEDY                                          86\n",
       "BREYER                                           85\n",
       "STEVENS                                          79\n",
       "SOUTER                                           77\n",
       "O'CONNOR                                         52\n",
       "REHNQUIST                                        49\n",
       "ROBERTS                                          38\n",
       "ALITO                                            32\n",
       "SOTOMAYOR                                         7\n",
       "STEVENS  AND  O'CONNOR ;  REHNQUIST ;  BREYER     2\n",
       "JUSTICE  ALITO                                    1\n",
       "BREYER ;                                          1\n",
       "ROBERTS ,  SCALIA ,  THOMAS ,  ALITO              1\n",
       "STEVENS  (IN PART),  BREYER  (IN PART)            1\n",
       "Name: authorship, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['authorship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:18.934021Z",
     "start_time": "2022-02-21T10:00:18.927074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SCALIA', 'GINSBURG', 'THOMAS', 'KENNEDY', 'BREYER', 'STEVENS',\n",
      "       'SOUTER', 'O'CONNOR', 'REHNQUIST', 'ROBERTS', 'ALITO'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# keep all judges through ALITO\n",
    "keep_judges = df['authorship'].value_counts().index[:11]\n",
    "print(keep_judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:19.088971Z",
     "start_time": "2022-02-21T10:00:19.078813Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['authorship'].isin(keep_judges)]\n",
    "df['authorship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:19.223647Z",
     "start_time": "2022-02-21T10:00:19.214366Z"
    }
   },
   "outputs": [],
   "source": [
    "df.date_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:19.372467Z",
     "start_time": "2022-02-21T10:00:19.358818Z"
    }
   },
   "outputs": [],
   "source": [
    "df['date_standard'] = pd.to_datetime(df['date_standard'])\n",
    "df['date_standard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:19.508463Z",
     "start_time": "2022-02-21T10:00:19.500655Z"
    }
   },
   "outputs": [],
   "source": [
    "df['year'] = df['date_standard'].dt.year\n",
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:20.410551Z",
     "start_time": "2022-02-21T10:00:19.630223Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cite_count'].hist(bins = 10000)\n",
    "plt.xlim([0,100000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:20.525929Z",
     "start_time": "2022-02-21T10:00:20.420551Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_cite_count'] = np.log(df['cite_count'])\n",
    "df['log_cite_count'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save what we have done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:21.806176Z",
     "start_time": "2022-02-21T10:00:20.538770Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('sc_cases_cleaned.pkl',compression='gzip')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over documents in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we show how to iterate over a dataframe and three different ways of how to tokenize documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:22.939652Z",
     "start_time": "2022-02-21T10:00:21.845650Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# more infos at https://spacy.io/\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:33.110230Z",
     "start_time": "2022-02-21T10:00:22.940665Z"
    }
   },
   "outputs": [],
   "source": [
    "processed = {} # empty python dictionary for processed data\n",
    "# iterate over rows\n",
    "for i, row in df.iterrows():\n",
    "    if i >= 10:\n",
    "        break\n",
    "    docid = i # make document identifier\n",
    "    text = row['opinion_text']     # get text snippet\n",
    "    document = nlp(text) # get sentences/tokens\n",
    "    processed[docid] = document # add to dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:33.112913Z",
     "start_time": "2022-02-21T10:00:33.111165Z"
    }
   },
   "outputs": [],
   "source": [
    "# first and second opinions\n",
    "print (\"opinion 1:\", processed[0][:50], \"\\n\\n\", \"opinion 2:\", processed[1][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see in more detail what information we can extract from documents procesesd using spaCy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:33.117334Z",
     "start_time": "2022-02-21T10:00:33.114138Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in processed[0][:50]:\n",
    "       print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we can preprocess with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:33.750426Z",
     "start_time": "2022-02-21T10:00:33.118057Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "processed = {} # empty python dictionary for processed data\n",
    "# iterate over rows\n",
    "for i, row in df.iterrows():\n",
    "    docid = i # make document identifier\n",
    "    text = row['opinion_text']     # get text snippet\n",
    "    document = simple_preprocess(text) # get sentences/tokens\n",
    "    processed[docid] = document # add to dictionary    \n",
    "    if i > 100:\n",
    "        break\n",
    "# first and second opinions\n",
    "print (\"opinion 1:\", processed[0][:50], \"\\n\\n\", \"opinion 2:\", processed[1][:50]) # note how simple preprocess drops punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:36.109926Z",
     "start_time": "2022-02-21T10:00:33.751232Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "processed = {} # empty python dictionary for processed data\n",
    "# iterate over rows\n",
    "for i, row in df.iterrows():\n",
    "    docid = i # make document identifier\n",
    "    text = row['opinion_text']     # get text snippet\n",
    "    document = word_tokenize(text.lower()) # get sentences/tokens\n",
    "    processed[docid] = document # add to dictionary    \n",
    "    if i > 100:\n",
    "        break\n",
    "# first and second opinions\n",
    "print (\"opinion 1:\", processed[0][:50], \"\\n\\n\", \"opinion 2:\", processed[1][:50]) # note that we just tokenize and keep all tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:36.166002Z",
     "start_time": "2022-02-21T10:00:36.110701Z"
    }
   },
   "outputs": [],
   "source": [
    "# save as python pickle\n",
    "pd.to_pickle(processed, 'processed_corpus.pkl')\n",
    "# delete it\n",
    "import os \n",
    "os.remove('processed_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:36.168077Z",
     "start_time": "2022-02-21T10:00:36.166895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging Data-frames Example\n",
    "# Perform a left join:\n",
    "# df_merged = pd.merge(df1,df2,on='id', how='left', validation='m:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.511559Z",
     "start_time": "2022-02-21T10:00:36.170185Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request as urllib # Python's module for accessing web pages\n",
    "url = 'https://goo.gl/VRF8Xs' # shortened URL for court case\n",
    "page = urllib.urlopen(url) # open the web page\n",
    "\n",
    "html = page.read() # read web page contents as a string\n",
    "print(html[:400])  # print first 400 characters\n",
    "print()\n",
    "print(html[-400:]) # print last 400 characters\n",
    "print()\n",
    "print(len(html),'characters in string.')   # print length of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.619993Z",
     "start_time": "2022-02-21T10:00:37.529459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse raw HTML\n",
    "# !pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup # package for parsing HTML\n",
    "soup = BeautifulSoup(html, 'lxml') # parse html of web page\n",
    "print(soup.title) # example usage: print title item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.624401Z",
     "start_time": "2022-02-21T10:00:37.620746Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract text\n",
    "text = soup.get_text() # get text (remove HTML markup)\n",
    "lines = text.splitlines() # split string into separate lines\n",
    "print(len(lines)) # print number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.627015Z",
     "start_time": "2022-02-21T10:00:37.625246Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = [line for line in lines if line != ''] # drop empty lines\n",
    "print(len(lines)) # print number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.629435Z",
     "start_time": "2022-02-21T10:00:37.627772Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lines[:20]) # print first 20 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.633928Z",
     "start_time": "2022-02-21T10:00:37.630296Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install unidecode\n",
    "from unidecode import unidecode # package for removing unicode\n",
    "uncode_str = 'Visualizations\\xa0'\n",
    "fixed = unidecode(uncode_str) # example usage\n",
    "print([uncode_str],[fixed]) # print cleaned string (replaced with a space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantity of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.860698Z",
     "start_time": "2022-02-21T10:00:37.634776Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_words_per_doc(txt):\n",
    "    # split text into words and count them.\n",
    "    return len(txt.split()) \n",
    "\n",
    "# apply to our dataframe\n",
    "df['num_words'] = df['opinion_text'].apply(get_words_per_doc)\n",
    "df['num_words'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:37.915240Z",
     "start_time": "2022-02-21T10:00:37.861729Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot length by year\n",
    "ax = df.groupby('year')['num_words'].mean().plot()\n",
    "ax.set_ylabel('Average Opinion Length')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:38.089132Z",
     "start_time": "2022-02-21T10:00:37.916113Z"
    }
   },
   "outputs": [],
   "source": [
    "df['log_words'] = np.log(df['num_words'])\n",
    "import seaborn as sns\n",
    "sns.jointplot(data=df,x='year', y='log_words',kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a frequency distribution over words with `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:38.181309Z",
     "start_time": "2022-02-21T10:00:38.090087Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "freqs = Counter()\n",
    "for i, row in df.iterrows():\n",
    "    freqs.update(row['opinion_text'].lower().split())\n",
    "    if i > 100:\n",
    "        break\n",
    "freqs.most_common()[:20] # can use most frequent words as style/function words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute (Number of documents) / (number of words per document). The google text classification guide recommends using N-Grams if this number is below 1500, or embedded sequences if this number is above 1500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:38.184275Z",
     "start_time": "2022-02-21T10:00:38.182079Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape[0] / df['num_words'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Dictionary / Matching Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:00:38.201317Z",
     "start_time": "2022-02-21T10:00:38.185161Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "print (spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:49.124573Z",
     "start_time": "2022-02-21T10:17:48.194664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary-Based Sentiment Analysis\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# textblob sentiment analysis: https://github.com/sloria/TextBlob\n",
    "# pip install spacytextblob\n",
    "\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# spacy_text_blob = SpacyTextBlob()\n",
    "nlp.add_pipe('spacytextblob')\n",
    "doc = nlp(df.iloc[0][\"opinion_text\"])\n",
    "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#sid = SentimentIntensityAnalyzer()\n",
    "#polarity = sid.polarity_scores(text)\n",
    "print(\"polarity\", doc._.polarity) # sentimentintensityanalayzer nltk: {'neg': 0.134, 'neu': 0.785, 'pos': 0.081, 'compound': -0.9999}\n",
    "print (\"subjectivity\", doc._.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:27.945292Z",
     "start_time": "2022-02-21T10:17:49.125544Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample 10% of the dataset\n",
    "dfs = df.sample(frac=.1) \n",
    "# apply compound sentiment score to data-frame\n",
    "def get_sentiment(snippet):\n",
    "    #return sid.polarity_scores(snippet)['compound']\n",
    "    return nlp(snippet)._.polarity\n",
    "dfs['sentiment'] = dfs['opinion_text'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:27.951003Z",
     "start_time": "2022-02-21T10:19:27.946118Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs.sort_values('sentiment',inplace=True)\n",
    "# print beginning of most positive documents\n",
    "[x[50:150] for x  in dfs[-5:]['opinion_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:27.955415Z",
     "start_time": "2022-02-21T10:19:27.952783Z"
    }
   },
   "outputs": [],
   "source": [
    "# print beginning of most negative documents\n",
    "[x[50:150] for x  in dfs[:5]['opinion_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:33.499886Z",
     "start_time": "2022-02-21T10:19:27.956283Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# sample 20% of the dataset\n",
    "dfs = df.sample(frac=.1) \n",
    "\n",
    "# apply compound sentiment score to data-frame\n",
    "def get_sentiment(snippet):\n",
    "    return sid.polarity_scores(snippet)['compound']\n",
    "dfs['sentiment_vader'] = dfs['opinion_text'].apply(get_sentiment)\n",
    "dfs.sort_values('sentiment_vader',inplace=True)\n",
    "# print beginning of most positive documents\n",
    "[x[50:150] for x  in dfs[-5:]['opinion_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:40.415038Z",
     "start_time": "2022-02-21T10:19:33.500578Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "pipe = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.638756Z",
     "start_time": "2022-02-21T10:19:40.417002Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class OpinionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return df.iloc[i][\"opinion_text\"][:512] #\n",
    "\n",
    "\n",
    "dataset = OpinionDataset(df)\n",
    "sentiments = []\n",
    "\n",
    "for out in tqdm(pipe(dataset, batch_size=16), total=len(dataset)):\n",
    "        if out['label'] == \"NEGATIVE\":\n",
    "            sentiments.append(-1*out['score'])\n",
    "        else:\n",
    "            sentiments.append(out['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.641320Z",
     "start_time": "2022-02-21T10:19:55.639504Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sentiments'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.645378Z",
     "start_time": "2022-02-21T10:19:55.641983Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values('sentiments',inplace=True)\n",
    "# print beginning of most positive documents\n",
    "[x[50:150] for x  in df[-5:]['opinion_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.649093Z",
     "start_time": "2022-02-21T10:19:55.647067Z"
    }
   },
   "outputs": [],
   "source": [
    "# print beginning of most negative documents\n",
    "[x[50:150] for x  in df[:5]['opinion_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.651335Z",
     "start_time": "2022-02-21T10:19:55.649820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#stopwords = set(stopwords.words('english'))\n",
    "#stopwords\n",
    "from spacy.lang.en import stop_words\n",
    "print (stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.654433Z",
     "start_time": "2022-02-21T10:19:55.652082Z"
    }
   },
   "outputs": [],
   "source": [
    "#stopfreq = np.sum([freqs[x] for x in stopwords])\n",
    "#stopfreq # 174132 for NLTK stopwords\n",
    "stopwords = stop_words.STOP_WORDS\n",
    "stopfreq = np.sum([freqs[x] for x in stopwords])\n",
    "stopfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:19:55.663598Z",
     "start_time": "2022-02-21T10:19:55.655124Z"
    }
   },
   "outputs": [],
   "source": [
    "otherfreq = np.sum([freqs[x] for x in freqs if x not in stopwords])\n",
    "otherfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [RegExOne Regular Expressions Lessons](regexone.com) and [the python documentation](https://docs.python.org/3/howto/regex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:35.191892Z",
     "start_time": "2022-02-21T10:17:35.163596Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "docs = dfs[:5]['opinion_text']\n",
    "\n",
    "# Extract words after justice.\n",
    "for doc in docs:    \n",
    "    print(re.findall(r'Justice \\w+ ', # pattern to match. always put 'r' in front of string so that backslashes are treated literally.\n",
    "                     doc,            # string\n",
    "                     re.IGNORECASE))  # ignore upper/lowercase (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:35.344169Z",
     "start_time": "2022-02-21T10:17:35.319988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract hyphenated words\n",
    "for doc in docs:    \n",
    "    print(re.findall(r'[a-z]+-[a-z]+', \n",
    "                     doc,            \n",
    "                     re.IGNORECASE))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:35.504081Z",
     "start_time": "2022-02-21T10:17:35.475760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract citations\n",
    "for i, doc in enumerate(docs):\n",
    "    finder = re.finditer('\\d+ [^\\s]+ \\d+', # pattern to match ([^\\s] means non-white-space)\n",
    "                     doc)            # string\n",
    "    for m in finder: \n",
    "        print(i, m.span(),m.group()) # location (start,end) and matching string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:35.615308Z",
     "start_time": "2022-02-21T10:17:35.609414Z"
    }
   },
   "outputs": [],
   "source": [
    "# baker-bloom economic uncertainty\n",
    "pattern1 = r'(\\b)uncertain[a-z]*'\n",
    "pattern2 = r'(\\b)econom[a-z]*'\n",
    "pattern3 = r'(\\b)congress(\\b)|(\\b)deficit(\\b)|(\\b)federal reserve(\\b)|(\\b)legislation(\\b)|(\\b)regulation(\\b)|(\\b)white house(\\b)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:35.756160Z",
     "start_time": "2022-02-21T10:17:35.748804Z"
    }
   },
   "outputs": [],
   "source": [
    "re.search(pattern1,'The White House tried to calm uncertainty in the markets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T10:17:35.900203Z",
     "start_time": "2022-02-21T10:17:35.891428Z"
    }
   },
   "outputs": [],
   "source": [
    "re.search(pattern2,'The Congress tried to calm uncertainty in the economy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(pattern3,'The Congress tried to calm uncertainty in the markets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(pattern3,'The Congress tried to calm uncertainty in the markets.', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicates_uncertainty(doc):\n",
    "    m1 = re.search(pattern1, doc, re.IGNORECASE)\n",
    "    m2 = re.search(pattern2, doc, re.IGNORECASE)\n",
    "    m3 = re.search(pattern3, doc, re.IGNORECASE)\n",
    "    if m1 and m2 and m3:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicates_uncertainty('The White House tried to calm uncertainty in the economy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicates_uncertainty('The White House tried to calm uncertainty in the markets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['uncertainty'] = df['opinion_text'].apply(indicates_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.uncertainty.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year')['uncertainty'].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples are based on the [NLTK tutorial](https://www.nltk.org/howto/wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('judge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('judge', pos='v') # can filter on part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = wn.synset('judge.n.01')\n",
    "judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('estimate.v.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories to which \"judge.n.01\" belongs\n",
    "judge.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the root category of \"judge.n.01\"\n",
    "judge.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('estimate.v.01').root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# members of the \"judge.n.01\" category\n",
    "judge.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"holonym\" is a part of a whole\n",
    "juror = wn.synset('juror.n.01')\n",
    "juror.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can find \"lowest common hypernyms\":\n",
    "judge.lowest_common_hypernyms(juror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"lemmas\" are specific senses of a specific word.\n",
    "judge.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemma.name() for lemma in judge.lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas have additional properties\n",
    "judge_lemma = judge.lemmas()[0]\n",
    "judge_lemma.derivationally_related_forms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = wn.synset('good.a.01').lemmas()[0]\n",
    "good.antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good.pertainyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb frames summarize the different semantic contexts that a verb can be used\n",
    "judge_verb = wn.synset('estimate.v.01').lemmas()[4]\n",
    "judge_verb.frame_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure similarity in the dictionary between words\n",
    "judge.path_similarity(juror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge.path_similarity(wn.synset('dog.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wu-Palmer similarity.\n",
    "judge.wup_similarity(juror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge.wup_similarity(wn.synset('dog.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can iterate over all synsets; e.g., all nouns:\n",
    "for synset in list(wn.all_synsets('n')):\n",
    "    if 'judg' in str(synset):\n",
    "        print(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Use wordnet to expand the set of words in the Baker-Bloom-Davis dictionary and re-compute policy uncertainty scores by year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Week 01. Text Data Essentials",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
